The Transformer model uses `self-attention` to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively.
* multi-head self-attention mechanism
* feed-forward neural network
* residual connections
* layer normalization
